{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a91ca-06c4-417b-991c-400cc4324f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure these libraries are available within your python environement.\n",
    "# Uncomment the following lines to install the libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install 'sagemaker>=2.42.0'\n",
    "!pip install jsonlines\n",
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install 'sagemaker[local]' --upgrade\n",
    "!pip install boto3\n",
    "!pip install pandas\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b48cc-ccbd-47cf-b4b3-db38eef86f0f",
   "metadata": {},
   "source": [
    "## 1 - GroundTruth annotation to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821338ef-f409-42ac-ae76-82e7477da9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"<YOUR GROUNDTRUTH S3 BUCKET>\"\n",
    "job_id = \"visualsearch\"\n",
    "gt_job_name = \"<YOUR GROUNDTRUTH JOB NAME>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a851c-79a6-4dc0-bced-07663d820097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "import s3fs\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def parse_gt_output(manifest_path, job_name):\n",
    "    \"\"\"\n",
    "    Captures the json Ground Truth bounding box annotations into a pandas dataframe\n",
    "\n",
    "    Input:\n",
    "    manifest_path: S3 path to the annotation file\n",
    "    job_name: name of the Ground Truth job\n",
    "\n",
    "    Returns:\n",
    "    df_bbox: pandas dataframe with bounding box coordinates\n",
    "             for each item in every image\n",
    "    \"\"\"\n",
    "\n",
    "    filesys = s3fs.S3FileSystem()\n",
    "    with filesys.open(manifest_path) as fin:\n",
    "        annot_list = []\n",
    "        for line in fin.readlines():\n",
    "            record = json.loads(line)\n",
    "            if job_name in record.keys():  # is it necessary?\n",
    "                image_file_path = record[\"source-ref\"]\n",
    "                image_file_name = image_file_path.split(\"/\")[-1]\n",
    "                class_maps = record[f\"{job_name}-metadata\"][\"class-map\"]\n",
    "\n",
    "                imsize_list = record[job_name][\"image_size\"]\n",
    "                assert len(imsize_list) == 1\n",
    "                image_width = imsize_list[0][\"width\"]\n",
    "                image_height = imsize_list[0][\"height\"]\n",
    "\n",
    "                for annot in record[job_name][\"annotations\"]:\n",
    "                    left = annot[\"left\"]\n",
    "                    top = annot[\"top\"]\n",
    "                    height = annot[\"height\"]\n",
    "                    width = annot[\"width\"]\n",
    "                    class_name = class_maps[f'{annot[\"class_id\"]}']\n",
    "\n",
    "                    annot_list.append(\n",
    "                        [\n",
    "                            image_file_name,\n",
    "                            class_name,\n",
    "                            left,\n",
    "                            top,\n",
    "                            height,\n",
    "                            width,\n",
    "                            image_width,\n",
    "                            image_height,\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "        df_bbox = pd.DataFrame(\n",
    "            annot_list,\n",
    "            columns=[\n",
    "                \"img_file\",\n",
    "                \"category\",\n",
    "                \"box_left\",\n",
    "                \"box_top\",\n",
    "                \"box_height\",\n",
    "                \"box_width\",\n",
    "                \"img_width\",\n",
    "                \"img_height\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return df_bbox\n",
    "\n",
    "\n",
    "def save_df_to_s3(df_local, s3_bucket, destination):\n",
    "    \"\"\"\n",
    "    Saves a pandas dataframe to S3\n",
    "\n",
    "    Input:\n",
    "    df_local: Dataframe to save\n",
    "    s3_bucket: Bucket name\n",
    "    destination: Prefix\n",
    "    \"\"\"\n",
    "\n",
    "    csv_buffer = StringIO()\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "    df_local.to_csv(csv_buffer, index=False)\n",
    "    s3_resource.Object(s3_bucket, destination).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "def save_df_to_local_csv(df_local, file_destination):\n",
    "    df_local.to_csv(file_destination, index=False)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Performs the following tasks:\n",
    "1. Reads input from 'input.json'\n",
    "2. Parses the Ground Truth annotations and creates a dataframe\n",
    "3. Saves the dataframe to S3\n",
    "\"\"\"\n",
    "\n",
    "manifest_path = f\"s3://{s3_bucket}/{job_id}/{gt_job_name}/manifests/output/output.manifest\"\n",
    "\n",
    "df_annot = parse_gt_output(manifest_path, gt_job_name)\n",
    "save_df_to_local_csv(df_annot, './annot.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceca32d-a33d-4d40-b819-d687be9a02cd",
   "metadata": {},
   "source": [
    "## 2 - Simple filter to get similar number of annotation per object category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e0ce4-b456-4460-8070-70ef1471293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ann = pd.read_csv('annot.csv')\n",
    "\n",
    "p = df_ann[[\"img_file\",\"category\"]]\n",
    "\n",
    "print('-- Before filter')\n",
    "print(p.groupby([\"category\"]).count())\n",
    "\n",
    "screwdrivers = p[p[\"category\"] == \"screwdriver\" ]\n",
    "screwdriverPerFile = screwdrivers.groupby(['img_file']).count()\n",
    "screwdriverToRemove = screwdriverPerFile[screwdriverPerFile[\"category\"] > 3]\n",
    "\n",
    "r = df_ann.merge(screwdriverToRemove.drop('category', 1), how='outer', on='img_file', indicator=True).query('_merge == \"left_only\"').drop('_merge', 1)\n",
    "print('-- After filter')\n",
    "print(r[[\"img_file\",\"category\"]].groupby([\"category\"]).count())\n",
    "\n",
    "\n",
    "save_df_to_local_csv(r, './annot-filtered.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad5ac2-8f71-4419-9b56-aed4b5d3f600",
   "metadata": {},
   "source": [
    "## 3 - Converting data set meta data to Yolo format, saving them to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c016dd-f217-4207-a9e2-f27360a4a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "\n",
    "yolo_output = \"labels/train\"\n",
    "\n",
    "def annot_yolo(annot_file, cats):\n",
    "    \"\"\"\n",
    "    Prepares the annotation in YOLO format\n",
    "\n",
    "    Input:\n",
    "    annot_file: csv file containing Ground Truth annotations\n",
    "    ordered_cats: List of object categories in proper order for model training\n",
    "\n",
    "    Returns:\n",
    "    df_ann: pandas dataframe with the following columns\n",
    "            img_file int_category box_center_w box_center_h box_width box_height\n",
    "\n",
    "\n",
    "    Note:\n",
    "    YOLO data format: <object-class> <x_center> <y_center> <width> <height>\n",
    "    \"\"\"\n",
    "\n",
    "    df_ann = pd.read_csv(annot_file)\n",
    "\n",
    "    df_ann[\"int_category\"] = df_ann[\"category\"].apply(lambda x: cats.index(x))\n",
    "    df_ann[\"box_center_w\"] = df_ann[\"box_left\"] + df_ann[\"box_width\"] / 2\n",
    "    df_ann[\"box_center_h\"] = df_ann[\"box_top\"] + df_ann[\"box_height\"] / 2\n",
    "\n",
    "    # scale box dimensions by image dimensions\n",
    "    df_ann[\"box_center_w\"] = df_ann[\"box_center_w\"] / df_ann[\"img_width\"]\n",
    "    df_ann[\"box_center_h\"] = df_ann[\"box_center_h\"] / df_ann[\"img_height\"]\n",
    "    df_ann[\"box_width\"] = df_ann[\"box_width\"] / df_ann[\"img_width\"]\n",
    "    df_ann[\"box_height\"] = df_ann[\"box_height\"] / df_ann[\"img_height\"]\n",
    "\n",
    "    return df_ann\n",
    "\n",
    "\n",
    "def save_annots_to_s3(s3_bucket, prefix, df_local):\n",
    "    \"\"\"\n",
    "    For every image in the dataset, save a text file with annotation in YOLO format\n",
    "\n",
    "    Input:\n",
    "    s3_bucket: S3 bucket name\n",
    "    prefix: Folder name under s3_bucket where files will be written\n",
    "    df_local: pandas dataframe with the following columns\n",
    "              img_file int_category box_center_w box_center_h box_width box_height\n",
    "    \"\"\"\n",
    "\n",
    "    unique_images = df_local[\"img_file\"].unique()\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "    for image_file in unique_images:\n",
    "        df_single_img_annots = df_local.loc[df_local.img_file == image_file]\n",
    "        annot_txt_file = image_file.split(\".\")[0] + \".txt\"\n",
    "        destination = f\"{prefix}/{annot_txt_file}\"\n",
    "\n",
    "        csv_buffer = StringIO()\n",
    "        df_single_img_annots.to_csv(\n",
    "            csv_buffer,\n",
    "            index=False,\n",
    "            header=False,\n",
    "            sep=\" \",\n",
    "            float_format=\"%.4f\",\n",
    "            columns=[\n",
    "                \"int_category\",\n",
    "                \"box_center_w\",\n",
    "                \"box_center_h\",\n",
    "                \"box_width\",\n",
    "                \"box_height\",\n",
    "            ],\n",
    "        )\n",
    "        s3_resource.Object(s3_bucket, destination).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "\n",
    "def get_cats(json_file):\n",
    "    \"\"\"\n",
    "    Makes a list of the category names in proper order\n",
    "\n",
    "    Input:\n",
    "    json_file: s3 path of the json file containing the category information\n",
    "\n",
    "    Returns:\n",
    "    cats: List of category names\n",
    "    \"\"\"\n",
    "\n",
    "    filesys = s3fs.S3FileSystem()\n",
    "    with filesys.open(json_file) as fin:\n",
    "        line = fin.readline()\n",
    "        record = json.loads(line)\n",
    "        labels = [item[\"label\"] for item in record[\"labels\"]]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Performs the following tasks:\n",
    "1. Reads input from 'input.json'\n",
    "2. Collect the category names from the Ground Truth job\n",
    "3. Creates a dataframe with annotaion in YOLO format\n",
    "4. Saves a text file in S3 with YOLO annotations\n",
    "   for each of the labeled images\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "s3_path_cats = (f\"s3://{s3_bucket}/{job_id}/{gt_job_name}/annotation-tool/data.json\")\n",
    "categories = get_cats(s3_path_cats)\n",
    "print(\"\\n labels used in Ground Truth job: \")\n",
    "print(categories, \"\\n\")\n",
    "\n",
    "gt_annot_file = \"annot-filtered.csv\"\n",
    "s3_dir = f\"{job_id}/{yolo_output}\"\n",
    "print(f\"annotation files saved in = \", s3_dir)\n",
    "\n",
    "df_annot = annot_yolo(gt_annot_file, categories)\n",
    "\n",
    "print(df_annot)\n",
    "\n",
    "save_annots_to_s3(s3_bucket, s3_dir, df_annot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376969f3-8eb3-49b9-b8c5-06db1cd9aee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
